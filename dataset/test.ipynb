{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: torch.Size([16, 8, 32, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerEncoderLayerWithChannels(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, dim_feedforward=2048, dropout=0.1):\n",
    "        super(TransformerEncoderLayerWithChannels, self).__init__()\n",
    "        \n",
    "        # Multi-head Self-Attention\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout)\n",
    "        \n",
    "        # Feedforward Network\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, embedding_dim),\n",
    "        )\n",
    "        \n",
    "        # Layer Normalization\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src 的形状为 (batch_size, channels, num_tokens, embedding_dim)\n",
    "        batch_size, channels, num_tokens, embedding_dim = src.shape\n",
    "        \n",
    "        # 我们对每个 channel 独立应用 self-attention\n",
    "        outputs = []\n",
    "        for ch in range(channels):\n",
    "            # 对于每个 channel，进行 multi-head self-attention\n",
    "            src_ch = src[:, ch, :, :]  # 取出当前 channel 的数据，形状为 (batch_size, num_tokens, embedding_dim)\n",
    "            \n",
    "            # Self-Attention expects input as (num_tokens, batch_size, embedding_dim)\n",
    "            src_ch_transposed = src_ch.transpose(0, 1)  # 转置为 (num_tokens, batch_size, embedding_dim)\n",
    "            \n",
    "            # Self-Attention, output shape: (num_tokens, batch_size, embedding_dim)\n",
    "            attn_output, _ = self.self_attn(src_ch_transposed, src_ch_transposed, src_ch_transposed)\n",
    "            \n",
    "            # Residual Connection + Layer Normalization\n",
    "            src2 = src_ch_transposed + self.dropout(attn_output)\n",
    "            src2 = self.norm1(src2)\n",
    "            \n",
    "            # Feedforward Layer\n",
    "            src2_transposed = src2.transpose(0, 1)  # 转回 (batch_size, num_tokens, embedding_dim)\n",
    "            feedforward_output = self.feedforward(src2_transposed)\n",
    "            \n",
    "            # Residual Connection + Layer Normalization\n",
    "            src3 = src2_transposed + self.dropout(feedforward_output)\n",
    "            output = self.norm2(src3)\n",
    "            \n",
    "            # 将处理好的 channel 加入 outputs 列表\n",
    "            outputs.append(output.unsqueeze(1))  # (batch_size, 1, num_tokens, embedding_dim)\n",
    "        \n",
    "        # 拼接所有 channels\n",
    "        outputs = torch.cat(outputs, dim=1)  # 最终形状为 (batch_size, channels, num_tokens, embedding_dim)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# 测试 TransformerEncoderLayerWithChannels\n",
    "batch_size = 16\n",
    "channels = 8  # 相当于通道数\n",
    "num_tokens = 32\n",
    "embedding_dim = 128\n",
    "num_heads = 8\n",
    "\n",
    "# 输入数据 RF (batch_size, channels, num_tokens, embedding_dim)\n",
    "RF_input = torch.randn(batch_size, channels, num_tokens, embedding_dim)\n",
    "\n",
    "# 创建 TransformerEncoderLayerWithChannels 模块\n",
    "encoder_layer = TransformerEncoderLayerWithChannels(embedding_dim=embedding_dim, num_heads=num_heads)\n",
    "\n",
    "# 前向传播\n",
    "encoder_output = encoder_layer(RF_input)\n",
    "\n",
    "print(f\"Encoder output shape: {encoder_output.shape}\")  # 输出 (16, 8, 32, 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4697151564184547\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=[1,2,3,4]\n",
    "Mtot = np.random.uniform(a[0],a[1])\n",
    "print(Mtot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "few_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
